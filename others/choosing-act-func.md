# Choosing Activiation Function on Neural Network

![screenshot](E:\Projects\bangkit-machine-learning-specialization\others\choosing-activation-func.png)

know the 'y' value for the output

ReLU is the most common choice

ReLU is a faster learning than sigmoid function

Syntax:

![screenshot](E:\Projects\bangkit-machine-learning-specialization\others\how-to-implement-act-func.png)
