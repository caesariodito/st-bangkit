# Choosing Activiation Function on Neural Network

![screenshot](choosing-activation-func.png)

know the 'y' value for the output

ReLU is the most common choice

ReLU is a faster learning than sigmoid function

Syntax:

![screenshot](how-to-implement-act-func.png)
